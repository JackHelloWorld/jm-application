<?xml version="1.0" encoding="UTF-8"?>
<configuration scan="true" scanPeriod="60 seconds" debug="false">
	<property name="LOG_NAME" value="ADMINLOG" />
	<property name="LOG_INFO" value="${LOG_NAME}.info" />
	<property name="LOG_ERROR" value="${LOG_NAME}.error" />
	<!--输出到控制台 -->
	<appender name="CONSOLE" class="ch.qos.logback.core.ConsoleAppender">
		<!-- <filter class="ch.qos.logback.classic.filter.ThresholdFilter"> <level>ERROR</level> 
			</filter> -->
		<encoder>
			<pattern>%d{yyyy-MM-dd HH:mm:ss.SSS} %contextName [%thread] %-5level
				%logger{36} - %msg%n</pattern>
			<!-- <pattern>%green(%d{yyyy-MM-dd HH:mm:ss.SSS}) %highlight(%5p) %magenta([%t]) 
				%white(%-40.40logger{40}) %boldMagenta(%3.3L) %blue(:) %cyan(%m%n)</pattern> -->
			<charset>UTF-8</charset>
		</encoder>
	</appender>

	<!--输出到kafka -->
	<appender name="kafkaAppender"
		class="com.github.danielwegener.logback.kafka.KafkaAppender">
		<encoder charset="UTF-8"
			class="net.logstash.logback.encoder.LogstashEncoder">
			<customFields>{"appname":"JM-ADMIN"}</customFields>
			<includeMdc>true</includeMdc>
			<includeContext>true</includeContext>
			<throwableConverter
				class="net.logstash.logback.stacktrace.ShortenedThrowableConverter">
				<maxDepthPerThrowable>30</maxDepthPerThrowable>
				<rootCauseFirst>true</rootCauseFirst>
			</throwableConverter>
		</encoder>
		<topic>loges</topic>
		<keyingStrategy
			class="com.github.danielwegener.logback.kafka.keying.HostNameKeyingStrategy" />
		<deliveryStrategy
			class="com.github.danielwegener.logback.kafka.delivery.AsynchronousDeliveryStrategy" />
		<producerConfig>bootstrap.servers=192.168.16.128:9092</producerConfig>
		<!-- don't wait for a broker to ack the reception of a batch. -->
		<producerConfig>acks=0</producerConfig>
		<!-- wait up to 1000ms and collect log messages before sending them as 
			a batch -->
		<producerConfig>linger.ms=1000</producerConfig>
		<!-- even if the producer buffer runs full, do not block the application 
			but start to drop messages -->
		<!--<producerConfig>max.block.ms=0</producerConfig> -->
		<producerConfig>block.on.buffer.full=false</producerConfig>
		<!-- kafka连接失败后，使用下面配置进行日志输出 -->
		<appender-ref ref="CONSOLE" />
	</appender>

	<!--输出到文件 -->
	<appender name="LOG_INFO"
		class="ch.qos.logback.core.rolling.RollingFileAppender">
		<rollingPolicy class="ch.qos.logback.core.rolling.TimeBasedRollingPolicy">
			<fileNamePattern>${LOG_NAME}/${LOG_INFO}.%d{yyyy-MM-dd}.log
			</fileNamePattern>
			<maxHistory>30</maxHistory>
			<totalSizeCap>1GB</totalSizeCap>
		</rollingPolicy>
		<encoder>
			<pattern>%d{yyyy-MM-dd HH:mm:ss.SSS} %contextName [%thread] %-5level
				%logger{36} - %msg%n</pattern>
			<charset>UTF-8</charset>
		</encoder>
	</appender>

	<appender name="LOG_ERROR"
		class="ch.qos.logback.core.rolling.RollingFileAppender">
		<rollingPolicy class="ch.qos.logback.core.rolling.TimeBasedRollingPolicy">
			<fileNamePattern>${LOG_NAME}/${LOG_ERROR}.%d{yyyy-MM-dd}.log
			</fileNamePattern>
			<maxHistory>30</maxHistory>
			<totalSizeCap>1GB</totalSizeCap>
		</rollingPolicy>
		<encoder>
			<pattern>%d{yyyy-MM-dd HH:mm:ss.SSS} %contextName [%thread] %-5level
				%logger{36} - %msg%n</pattern>
			<charset>UTF-8</charset>
		</encoder>
	</appender>

	<root level="info">
		<appender-ref ref="CONSOLE" />
		<appender-ref ref="LOG_INFO" />
		<appender-ref ref="LOG_ERROR" />
		<appender-ref ref="kafkaAppender" />
	</root>

	<logger name="com.jm">
		<level value="debug" />
	</logger>

	<logger name="java.sql.Connection" level="DEBUG" />
	<logger name="java.sql.Statement" level="DEBUG" />
	<logger name="java.sql.PreparedStatement" level="DEBUG" />
</configuration>